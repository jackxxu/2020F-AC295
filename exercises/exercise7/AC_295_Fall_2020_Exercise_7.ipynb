{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AC 295 Fall 2020 Exercise 7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNoQXuvWxiOZ"
      },
      "source": [
        "<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n",
        "    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> AC295: Advanced Practical Data Science </h1>\n",
        "\n",
        "## Attention and Transformers\n",
        "\n",
        "**Harvard University, Fall 2020**  \n",
        "**Instructors**: Pavlos Protopapas  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79nMnGpqZkmf"
      },
      "source": [
        "**Each assignment is graded out of 5 points.  The topic for this assignment is Transfer Learning for Text.**\n",
        "\n",
        "**Due:** 11/03/2020 10:15 AM EDT\n",
        "\n",
        "**Submit:** We won't be re running your notebooks, please ensure output is visible in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ceFPHhfZlLq"
      },
      "source": [
        "#### Learning Objectives\n",
        "\n",
        "In this exercise you will cover the following topics:  \n",
        "- Tokenizing text for BERT\n",
        "- BERT for Text Classification Task\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTg9MWFYtEk7"
      },
      "source": [
        "#### Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2TpO4fPtGkv"
      },
      "source": [
        "!pip install transformers #Installing Huggingface transformers "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ehhLBsZtHH0"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtFGBRPHtMaZ"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import tarfile\n",
        "import shutil\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from string import Template\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJe8HU_CtRCW"
      },
      "source": [
        "#### Verify Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK7q5kurtSxs"
      },
      "source": [
        "# Enable/Disable Eager Execution\n",
        "# Reference: https://www.tensorflow.org/guide/eager\n",
        "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
        "# without building graphs\n",
        "\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "#tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "print(\"tensorflow version\", tf.__version__)\n",
        "print(\"keras version\", tf.keras.__version__)\n",
        "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
        "\n",
        "# Get the number of replicas \n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
        "\n",
        "devices = tf.config.experimental.get_visible_devices()\n",
        "print(\"Devices:\", devices)\n",
        "print(tf.config.experimental.list_logical_devices('GPU'))\n",
        "\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
        "\n",
        "# Better performance with the tf.data API\n",
        "# Reference: https://www.tensorflow.org/guide/data_performance\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mshzP5vetVqI"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkcpmAx5tW0a"
      },
      "source": [
        "def download_file(packet_url, base_path=\"\", extract=False):\n",
        "  if base_path != \"\":\n",
        "    if not os.path.exists(base_path):\n",
        "      os.mkdir(base_path)\n",
        "  packet_file = os.path.basename(packet_url)\n",
        "  with requests.get(packet_url, stream=True) as r:\n",
        "      r.raise_for_status()\n",
        "      with open(os.path.join(base_path,packet_file), 'wb') as f:\n",
        "          for chunk in r.iter_content(chunk_size=8192):\n",
        "              f.write(chunk)\n",
        "  \n",
        "  if extract:\n",
        "    if packet_file.endswith(\".zip\"):\n",
        "      with zipfile.ZipFile(os.path.join(base_path,packet_file)) as zfile:\n",
        "        zfile.extractall(base_path)\n",
        "    \n",
        "    if packet_file.endswith(\".tar.gz\"):\n",
        "      packet_name = packet_file.split('.')[0]\n",
        "      with tarfile.open(os.path.join(base_path,packet_file)) as tfile:\n",
        "        tfile.extractall(base_path)\n",
        "\n",
        "def evaluate_model(model,test_data, training_results):\n",
        "    \n",
        "  # Get the model train history\n",
        "  model_train_history = training_results.history\n",
        "  # Get the number of epochs the training was run for\n",
        "  num_epochs = len(model_train_history[\"loss\"])\n",
        "\n",
        "  # Plot training results\n",
        "  fig = plt.figure(figsize=(15,5))\n",
        "  axs = fig.add_subplot(1,2,1)\n",
        "  axs.set_title('Loss')\n",
        "  # Plot all metrics\n",
        "  for metric in [\"loss\",\"val_loss\"]:\n",
        "      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n",
        "  axs.legend()\n",
        "  \n",
        "  axs = fig.add_subplot(1,2,2)\n",
        "  axs.set_title('Accuracy')\n",
        "  # Plot all metrics\n",
        "  for metric in [\"accuracy\",\"val_accuracy\"]:\n",
        "      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n",
        "  axs.legend()\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  # Evaluate on test data\n",
        "  evaluation_results = model.evaluate(test_data)\n",
        "  print(\"Evaluation Results:\", evaluation_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adcr59aMC0x9"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "**We will continue to use the dataset from Exercise 6.** The dataset consists of news articles from CNN in the politics, health, and entertainment category. There are about 300 articles in each category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rhkLpzqtg8R"
      },
      "source": [
        "#### Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWeC7j6DtiYz"
      },
      "source": [
        "start_time = time.time()\n",
        "download_file(\"https://storage.googleapis.com/dataset_store/ac295/news300.zip\", base_path=\"datasets\", extract=True)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Download execution time (mins)\",execution_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW_MeOmStjoy"
      },
      "source": [
        "#### Explore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XrbTi03tmzU"
      },
      "source": [
        "data_dir = os.path.join(\"datasets\",\"news300\")\n",
        "label_names = os.listdir(data_dir)\n",
        "\n",
        "# Number of unique labels\n",
        "num_classes = len(label_names) \n",
        "# Create label index for easy lookup\n",
        "label2index = dict((name, index) for index, name in enumerate(label_names))\n",
        "index2label = dict((index, name) for index, name in enumerate(label_names))\n",
        "\n",
        "print(\"Number of classes:\", num_classes)\n",
        "print(\"Labels:\", label_names)\n",
        "\n",
        "# Generate a list of labels and path to text\n",
        "data_x = []\n",
        "data_y = []\n",
        "\n",
        "for label in label_names:\n",
        "  text_files = os.listdir(os.path.join(data_dir,label))\n",
        "  data_x.extend([os.path.join(data_dir,label,f) for f in text_files])\n",
        "  data_y.extend([label for f in text_files])\n",
        "\n",
        "# Load the text content\n",
        "for idx, path in enumerate(data_x):\n",
        "  # Load text\n",
        "  with open(path) as file:\n",
        "    data_x[idx] = file.read()\n",
        "\n",
        "# Preview\n",
        "print(\"data_x count:\",len(data_x))\n",
        "print(\"data_y count:\",len(data_y))\n",
        "print(data_x[:5])\n",
        "print(data_y[:5])\n",
        "print(\"Label counts:\",np.unique(data_y, return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQno1a1Atqjq"
      },
      "source": [
        "#### View Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpUXhNYptr9n"
      },
      "source": [
        "# Generate a random sample of index\n",
        "data_samples = np.random.randint(0,high=len(data_x)-1, size=10)\n",
        "for i,data_idx in enumerate(data_samples):\n",
        "  print(\"Label:\",data_y[data_idx],\", Text:\",data_x[data_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxhqIHteuffp"
      },
      "source": [
        "## Questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzRTSgwjhwom"
      },
      "source": [
        "For this exercise you will use a transformer model **BERT** that was discussed in lecture. [Hugging Face](https://huggingface.co/) is an NLP-focused company with a large open-source library especially around the Transformers. They provide the transformers library that exposes an API to use many well-known transformer architectures, such as BERT, RoBERTa, GPT-2 or DistilBERT, that obtain state-of-the-art results on a variety of NLP tasks like text classification, information extraction, question answering, and text generation. These architectures come pre-trained with several sets of weights. Getting started with Transformers only requires to install the `pip install transformers`\n",
        "\n",
        "You will use BERT direclty from the transformers package\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqyILrmJCvld"
      },
      "source": [
        "## Question 1 : Build a text classification model using BERT (3.0 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdhF9ODTXm7G"
      },
      "source": [
        "#### a) Prepare data for BERT\n",
        "\n",
        "BERT requires the data to be tokenized in a specific way, for this you need to use the `BertTokenizer` from the `transformers` package from Hugging Face. Steps to prepare your dataset:\n",
        "\n",
        "- Split data to train/validation\n",
        "- Use `BertTokenizer` to tokenize the input text\n",
        "- [BertTokenizer](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer), use `bert-base-uncased` as the `vocab_file` argument\n",
        "- When using `tokenizer.encode_plus(...)` use the `max_length=256` or some value `<=512`. You may run into OOM error during training if the value is high\n",
        "- The output tokens from `tokenizer.encode_plus(...)` is a dictionary with the keys `'input_ids', 'token_type_ids', 'attention_mask'`\n",
        "- Remember to convert the data y values `to_categorical`\n",
        "- Create TF Datasets using the tokenized results. When using `tf.data.Dataset.from_tensor_slices(...,...)` look out for the x values passed in. `BERT` requires 3 inputs `'input_ids', 'token_type_ids', 'attention_mask'` as a tuple.\n",
        "- Remember to apply `shuffle(..)` `batch(...)` `prefetch(..)` to your train and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StP1O5vEXx3a"
      },
      "source": [
        "# Examples on how to use the BertTokenizer\n",
        "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
        "\n",
        "# Tokenizer encode_plus\n",
        "text = \"What you need to know about using them safely amid the pandemic\"\n",
        "outputs = tokenizer.encode_plus(text, \n",
        "                  add_special_tokens = True, # add [CLS], [SEP]\n",
        "                  max_length = 15, # max length of the text that can go to BERT (<=512)\n",
        "                  padding='max_length',\n",
        "                  return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "                  truncation='longest_first',\n",
        "                  return_tensors=\"tf\"\n",
        "              )\n",
        "print(\"Tokenizer Output:\",outputs)\n",
        "\n",
        "# Tokenizer batch_encode_plus\n",
        "text = [\"What you need to know about using them safely amid the pandemic\", \n",
        "        \"A third of Medicare enrollees with coronavirus ended up in the hospital\"]\n",
        "outputs = tokenizer.batch_encode_plus(\n",
        "        text,\n",
        "        return_tensors='tf',\n",
        "        add_special_tokens = True, # add [CLS], [SEP]\n",
        "        return_token_type_ids=True,\n",
        "        padding='max_length',\n",
        "        max_length=15, # max length of the text that can go to BERT (<=512)\n",
        "        return_attention_mask = True,\n",
        "        truncation='longest_first'\n",
        "    )\n",
        "print(\"Tokenizer Output:\",outputs)\n",
        "print(\"Tokenizer Output Keys:\", outputs.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r65CIK8Cx6wY"
      },
      "source": [
        "# Datatset Params\n",
        "batch_size = 8 # You can try higher values but may run into OOM errors depending on which GPU you are using\n",
        "train_shuffle_buffer_size = 800\n",
        "validation_shuffle_buffer_size = 200\n",
        "\n",
        "# Convert all y labels to numbers\n",
        "\n",
        "# Converts to y to_categorical\n",
        "\n",
        "# Create TF Dataset\n",
        "\n",
        "# print(\"train_data\",train_data)\n",
        "# print(\"validation_data\",validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5Azr2qehjcL"
      },
      "source": [
        "Your train and validation dataset should look something like this\n",
        "```\n",
        "print(\"train_data\",train_data)\n",
        "print(\"validation_data\",validation_data)\n",
        "\n",
        "train_data <PrefetchDataset shapes: ({input_ids: (None, 256), token_type_ids: (None, 256), attention_mask: (None, 256)}, (None, 3)), types: ({input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n",
        "validation_data <PrefetchDataset shapes: ({input_ids: (None, 256), token_type_ids: (None, 256), attention_mask: (None, 256)}, (None, 3)), types: ({input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VitkmfJsDJY7"
      },
      "source": [
        "#### b) BERT for Sequence Classification\n",
        "\n",
        "- Build a model using `TFBertForSequenceClassification` from the `transformers` package from Hugging Face\n",
        "- Load the pre-trained weights using `bert-base-uncased` make sure to pass the argument `num_labels`\n",
        "- Train your model\n",
        "- Ensure there is a plot of your training history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U4u9xmOXmD7"
      },
      "source": [
        "############################\n",
        "# Training Params\n",
        "############################\n",
        "learning_rate = 2e-5 # Try 5e-5, 3e-5, 2e-5\n",
        "epochs = 5\n",
        "\n",
        "# Free up memory\n",
        "K.clear_session()\n",
        "\n",
        "# Build BERT model\n",
        "model = ... \n",
        "\n",
        "# Optimizer\n",
        "optimizer = optimizers.Adam(lr=learning_rate, epsilon=1e-08)\n",
        "\n",
        "# Loss\n",
        "\n",
        "# Compile\n",
        "\n",
        "# Train model\n",
        "\n",
        "# Evaluate Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RRozUGMYutx"
      },
      "source": [
        "#### c) Classification Results\n",
        "\n",
        "- What was your validation accuracy?\n",
        "- It should be more that 95%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Z4eB2-B2k5"
      },
      "source": [
        "---\n",
        "## Question 2 : Conceptual (2.0 Points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkPkjEEYB7ls"
      },
      "source": [
        "#### a) How does the encoder-decoder structure work for language modelling?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv_wB17JCSmS"
      },
      "source": [
        "*Your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7bQJ8M8CSvX"
      },
      "source": [
        "\n",
        "#### b) Explain in your own words what is the attention mechanism. Why do State of the art models use this concept?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0667seiCWwO"
      },
      "source": [
        "*Your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6zhU_TWCW6a"
      },
      "source": [
        "#### c) What is the biggest benefit of transformers compared to seqtoseq models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBrVqCMCY17"
      },
      "source": [
        "*Your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HksuGmznCZGp"
      },
      "source": [
        "#### d) Explain in your own words what the positional encoder is and why it is needed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmB3mNiChMY"
      },
      "source": [
        "*Your answer here*"
      ]
    }
  ]
}