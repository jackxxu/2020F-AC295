{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AC 295 Fall 2020 Exercise 8.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"aNoQXuvWxiOZ"},"source":["<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n","    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> AC295: Advanced Practical Data Science </h1>\n","\n","## Model Compression Techniques\n","\n","**Harvard University, Fall 2020**  \n","**Instructors**: Pavlos Protopapas  \n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"eR13y-s1Cpaj"},"source":["**Each assignment is graded out of 5 points.  The topic for this assignment is Distillation and Pruning.**\n","\n","**Due:** 11/10/2020 10:15 AM EDT\n","\n","**Submit:** We won't be re running your notebooks, please ensure output is visible in the notebook."]},{"cell_type":"markdown","metadata":{"id":"-nT1MipDCyFC"},"source":["#### Learning Objectives\n","\n","In this exercise you will cover the following topics:  \n","- Knowledge Distillation\n","- Distill Teacher to Student\n","- Model Pruning\n","\n","\n","This exercise aims to distill a mobile-net model that has been trained initially on imagenet and then trained on vegetables images. The learned mobilenet-base model will be considered the \"teacher\" network. You will use distillation techniques to train a smaller, less sophisticated network that is called a \"student\" network. The hope is that the wisdom from the teacher network will be distilled and used. \n","\n","Then you will learn how to prune the weights of a model using the `tensorflow_model_optimization` package \n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"KiwWD6R9q_Zw"},"source":["#### Installs"]},{"cell_type":"code","metadata":{"id":"jiZO-qOSrBIh","executionInfo":{"status":"ok","timestamp":1604544553913,"user_tz":300,"elapsed":4407,"user":{"displayName":"Pavlos Protopapas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFiMxWyBX0zyqHmZ6nl5pGDTgC2LfpnD075J8ZOdM=s64","userId":"00316814864071164174"}},"outputId":"76681822-b62a-4cd3-d31f-16f2684a88f9","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install -q tensorflow_model_optimization"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |██                              | 10kB 18.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 40kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 2.7MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k75n_nkWqJ__"},"source":["#### Imports"]},{"cell_type":"code","metadata":{"id":"RCThfbCzqMFL"},"source":["import os\n","import requests\n","import tempfile\n","import zipfile\n","import shutil\n","import json\n","import time\n","import sys\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from glob import glob\n","import subprocess\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.python.keras import backend as K\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras import layers\n","from tensorflow.keras import activations\n","from tensorflow.keras import optimizers\n","from tensorflow.keras import losses\n","from tensorflow.keras import metrics\n","from tensorflow.keras import initializers\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.utils import to_categorical\n","from keras.utils.layer_utils import count_params\n","import tensorflow_hub as hub\n","\n","import tensorflow_model_optimization as tfmot\n","from tensorflow_model_optimization.sparsity.keras import prune_low_magnitude\n","\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WthynN97DPy3"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"yuAUTFCCoodt"},"source":["**We will use the dataset from Exercise 4.** The dataset consists of images downloaded from Google Image search. There are 5 classes of the following labels: **'tomato', 'beetroot', 'broccoli', 'bell_pepper', 'carrot'**.  \n","\n","[Link to dataset](https://github.com/shivasj/dataset-store/releases/download/v1.0/vegetables.zip)"]},{"cell_type":"markdown","metadata":{"id":"2RxTokf4p3MK"},"source":["## Question 1 : Build Teacher Model (1.0 Point)\n","\n","Steps to build teacher model:\n","- Refer to code from Lecture Demo\n","- Download data & Create TF Datasets \n","- Build a transfer learning model to classify vegetables. If you use the TF Hub [mobilenet](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4) and `learning_rate = 0.001` and `epochs > 30` you should easliy be able to get a validation accuracy of 85% or higher\n","- Ensure there is a plot of your training history"]},{"cell_type":"code","metadata":{"id":"kIgP0FTAsFN7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mIMmsqi4qRXv"},"source":["## Question 2 : Build Smaller Student Model (1.0 Point)\n","\n","Steps to build teacher model:\n","- Refer to code from Lecture Demo\n","- Build a very small student model to classify vegetables. Use just 2 Convolution layers with max pooling and a dense layer\n","- Train the student model from scratch but use `learning_rate = 0.01` and `epochs = 10`\n","- Ensure there is a plot of your training history"]},{"cell_type":"code","metadata":{"id":"U7fBaGpBsF0g"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GuTRHEjJqjlH"},"source":["## Question 3 : Model Distillation (1.5 Points)\n","\n","Steps to distill teacher to student:\n","- Refer to code from Lecture Demo\n","- Copy the `Distiller` class over from lecture demo\n","- Keeping `learning_rate = 0.01` and `epochs = 10` constant, distill teacher model to student model as shown in the demo code\n","- You will notice there are few new parameters when you compile the `Distiller` model:\n","  - **Student Loss Function**: `student_loss_fn`. Set this to `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`\n","  - **Distillation Loss Function**: `distillation_loss_fn`. Set this to `tf.keras.losses.KLDivergence()`\n","  - **Alpha**: alpha to `student_loss_fn` and 1-alpha to `distillation_loss_fn`\n","  - **Temperature**: Temperature for softening probability distributions. The larger the temperature gives softer the distributions\n","- Try out various values for `alpha` ranging from `[0.1,0.2,0.3,0.5,1.0]`\n","- Try out various values for `temperature` e.g: `[1,5,10,15,30]`\n","- Plot the validation accuracy for the various values of `alpha` you tried\n","- Plot the validation accuracy for the various values of `temperature` you tried\n","- Pick the best `alpha` and `temperature` and train your final student model\n","- Ensure there is a plot of your training history of the final student model\n","- What are your **model size**, **total parameters**, and **accuracy** of your teacher model, student model trained with distillation, and student model trained from scratch. Feel free to use the util functions from the demo code"]},{"cell_type":"code","metadata":{"id":"-oFN5qZIsGdA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2bs0YTAts2MN"},"source":["## Question 4 : Model Pruning (1.5 Point)\n","\n","In this question you will use the already trained model from the previous question and perform some weights pruning.  \n","\n","Steps to perform model pruning:  \n","- You will use the package `tensorflow_model_optimization`, `!pip install -q tensorflow_model_optimization` which has already been included in the notebook\n","- For this problem you will use the student model from scratch you already trained in question 3\n","- Here are some helper functions to view model weights:"]},{"cell_type":"code","metadata":{"id":"TIU_yo-cYdF1"},"source":["def check_model_weights(model):\n","  for i, w in enumerate(model.get_weights()):\n","    print(model.weights[i].name,\"Total:\",w.size, \"Zeros:\", round(np.sum(w == 0) / w.size * 100,2),\"%\")\n","\n","def compare_model_sizes(model):\n","    _, model_file = tempfile.mkstemp(\".h5\")\n","    tf.keras.models.save_model(model, model_file, include_optimizer=False)\n","    _, zip3 = tempfile.mkstemp(\".zip\")\n","    with zipfile.ZipFile(zip3, \"w\", compression=zipfile.ZIP_DEFLATED) as f:\n","        f.write(model_file)\n","    print(\"Model before zipping: %.2f Kb\"% (os.path.getsize(model_file) / float(1000)))\n","    print(\"Model after zipping: %.2f Kb\"% (os.path.getsize(zip3) / float(1000)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ge9wVyH8Zyaw"},"source":["- Run `check_model_weights(...)` and `compare_model_sizes(...)` on your student model from scratch"]},{"cell_type":"code","metadata":{"id":"dfW5hyNGZxHX"},"source":["# Check model before pruning\n","check_model_weights(...)\n","compare_model_sizes(...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ltBHgFFKZDuI"},"source":["- Next you will perform model pruning. For this you will need to create a wrapper model that performs the pruning. YOu can do this by passing the student model into the `prune_low_magnitude` function as shown. This is function is from the `tensorflow_model_optimization` package\n","- Compile the new model, `model_for_pruning` with the same optimizer and loss function that you used to train the student model from scratch\n","- Add a pruning callback\n","- Train your `model_for_pruning` model for just 2 epochs. This is enough for the wrapper model to prune weights of the the actual model"]},{"cell_type":"code","metadata":{"id":"Ms9SzQhyYxRZ"},"source":["# Define model for pruning\n","epochs = 2\n","end_step = np.ceil(len(train_x) / batch_size).astype(np.int32) * epochs\n","pruning_params = {\n","      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n","                                                               final_sparsity=0.80,\n","                                                               begin_step=0,\n","                                                               end_step=end_step)\n","}\n","model_for_pruning = prune_low_magnitude(before_prune, **pruning_params)\n","\n","# Optimizer\n","optimizer = ...\n","# Loss\n","loss = ...\n","# Compile model_for_pruning\n","\n","# Callback\n","callbacks = [\n","  tfmot.sparsity.keras.UpdatePruningStep()\n","]\n","\n","# Train\n","start_time = time.time()\n","training_results = model_for_pruning.fit(\n","        train_data,\n","        validation_data=validation_data,\n","        epochs=epochs,\n","        callbacks=callbacks,\n","        verbose=1)\n","execution_time = (time.time() - start_time)/60.0\n","print(\"Training execution time (mins)\",execution_time)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4vbGpPKqZgld"},"source":["- Next you will need to get the student model from scratch back from the pruning wrapper. So for this you will use"]},{"cell_type":"code","metadata":{"id":"YraOsUz0vL_g"},"source":["# Get the model back after pruning\n","after_prune = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n","after_prune.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mxz85RlgZkPE"},"source":["- Now `after_prune` is your pruned model (of the original student model from scratch)\n","- Run `check_model_weights(...)` and `compare_model_sizes(...)` on your pruned model\n","- Compare what you see from `check_model_weights(...)` on your student model from scratch vs. pruned model\n","- Compare what you see from `compare_model_sizes(...)` on your student model from scratch vs. pruned model"]},{"cell_type":"code","metadata":{"id":"vDR59x7gZlGt"},"source":["# Check model after pruning\n","check_model_weights(after_prune)\n","compare_model_sizes(after_prune)"],"execution_count":null,"outputs":[]}]}